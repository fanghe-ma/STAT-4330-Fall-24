\documentclass[a4paper, 10pt]{report}
\usepackage[margin = 1in]{geometry}
\usepackage{amsmath}
\usepackage{tabularx}
\usepackage{framed}
\usepackage{array}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{titlesec}
\titleformat{\chapter}[hang]
{\normalfont\huge\bfseries}{\chaptertitlename\ \thechapter:}{1em}{}

\newcommand\smallmath[2]{#1{\raisebox{\dimexpr \fontdimen 22 \textfont 2
      - \fontdimen 22 \scriptscriptfont 2 \relax}{$\scriptscriptstyle #2$}}}
\newcommand\smalloplus{\smallmath\mathbin\oplus}

\setlength{\parindent}{0em}
\newcolumntype{L}{>{\arraybackslash}m{10cm}}
\newcolumntype{T}{>{\arraybackslash}m{6cm}}


\begin{document}

\title{Linear Algebra Done Right - Notes}
\maketitle
\tableofcontents

\chapter{Vector Spaces}

\section{Complex Numbers}

\begin{framed}
   \textbf{Definition}: a complex number is an \textbf{ordered pair} $(a, b)$ where $a, b \in \mathbb{R}$. The set of all complex numbers is $C$ 
   \[
     C = \{ a + bi: a, b \in \mathbb{R} \} 
   \] 
\end{framed}

Properties of complex numbers
\begin{itemize}
   \item commutative
      \[
        w + z = z + w \text{ and } wz = zw
      \] 
   \item associativity
      \[
         z_1 + (z_2 + z_3) = (z_1 + z_2) + z_3 \text{ and } (z_1z_2) z_3  = z_1 (z_2 z_3)
      \] 
   \item identities
      \[
        z + 0 = z \text{ and } z 1 = z
      \] 
   \item additive inverse
      \[
        \forall z \in C \exists w \in C s.t. z + w = 0
      \]  
   \item multiplicative inverse
   \item distributive
\end{itemize}

  
\section{Vector spaces}

Let $F$ denote the set of all real and complex numbers. $F^n$ denotes a list of $n$ elements. 
\[
   F^n = \{ (x_1, \hdots x_n): x_j \in F \text{ for } j = 1, \hdots n\} 
\] 

\begin{framed}
   \textbf{Definition}: A \textbf{vector space} is a set $V$ along with addition on $V$ and scalar multiplication on $V$ such that the following properties hold
   \begin{itemize}
      \item commutativity
      \item associativity
      \item additive identity
      \item additive inverse
      \item multiplicative identity
      \item distributive properties
   \end{itemize}
\end{framed}

\textcolor{red}{NOTE:} Scalar multiplication in vector space depends on $F$. Therefore we say $V$ is a vector space over $F$. 

\section{Properties of vector spaces}

Proposition 1.2: A vector space has a unique additive identity. \\
Proposition 1.3: Each element in a vector space has a unique additive inverse\\
Proposition 1.4: $0v = 0$ for every  $v \in V$\\
Proposition 1.5: $a_0 = 0$ for every  $a \in F$\\
Proposition 1.6: $(-1) v = -v $ for every  $v \in V$\\

\section{Subspaces}
\begin{framed}
   \textbf{Definition}: A subset $U$ of $V$ is a subspace of $V$ if $U$ is also a vector space. \\

   To proof $U$ is a subspace, show
   \begin{itemize}
      \item additive identity ($U$ contains 0) 
      \item closed under addition
      \item closed under scalar multiplication
     
   \end{itemize}
\end{framed}

\section{Sums and direct sums}

For $U_1, \hdots U_m$ that are subspaces of $V$, the sum of $U_1, \hdots U_m$ denoted $U_1 + U_2 \hdots U_m$ is defined to be the set of all possible sums of $U_1, \hdots U_m$ 
\[
  U_1 + U_2 \hdots U_m = \{  u_1 + \hdots u_m: u_1 \in U_1, \hdots u_m \in U_m \} 
\] 

\begin{framed}
   \textbf{Definition}: $V$ is the direct sum of subspaces $U_1, U_2 \hdots U_m$ if each element of $V$ can be written uniquely as a sum $u_1 $
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $U_1, \hdots U_n$ are subspaces of $V$, then $V = U_1 \smalloplus U_2 \hdots \smalloplus U_n$ if and only if
   \begin{enumerate}
      \item $V_1 = U_1 + \hdots U_n$ 
      \item the only way to write $0$ as a sum of $u_1 + \hdots u_n$ is by taking all $u_j$'s equal to $0$
   \end{enumerate}
\end{framed}

\begin{framed}
   \textbf{Proposition}: Suppose that $U$ and $W$ are subspaces of $V$, then $V = U \smalloplus W$ iff  $V = U + W$ and $U \cap W = \{ 0 \} $
\end{framed}

\chapter{Finite dimensional vector spaces}

\section{Span and linear independence}

\begin{framed}
  \textbf{Definition}: a \textbf{linear combination} of a list $(v_1, \hdots v_m)$ of vectors in $V$ is a vector of the form
  \[
     a_1 v_1 + \hdots a_m v_m
  \] 
   where $a_1, \hdots a_m \in F$
\end{framed}

\begin{framed}
   \textbf{Definition}: The set of all linear combinations of $(v_1, \hdots v_m)$ is the \textbf{span} of $(v_1, \hdots v_m)$ 
   \[
     span(v_1, \hdots, v_m) = \{ a_1v_1 + \hdots a_m v_m: a_1 \hdots a_m \in F \} 
   \] 
\end{framed}

\begin{framed}
   \textbf{Definition}: A list $(v_1, \hdots v_m)$ of vectors in $V$ is called \textbf{linearly independent} if the only choice of $a_1, \hdots a_m \in F$ that makes $a_1v_1 + \hdots + a_m v_m$ equal to $0$ is $a_1 = \hdots = a_m = 0$
\end{framed}

Removing a vector from a linearly independent list yields another linearly independent list. 

\begin{framed}
   \textbf{Linear Independence Lemma}: If $(v_1, \hdots, v_m)$ is linearly dependent in $V$ and $v_1 \neq 0$, then there exists $j \in \{ 2, \hdots, m \} $ such that 
   \begin{enumerate}
      \item $v_j \in span(v_1, \hdots v_{j-1})$ 
      \item if the $j$-th term is removed from $(v_1, \hdots, v_m)$ the span of the remaining list equals $span(v_1, \hdots, v_m)$
     
   \end{enumerate}
\end{framed}

\section{Bases}
\begin{framed}
   \textbf{Definition}: A \textbf{basis} of $V$ is a list of vectors in $V$ that is linearly independent and spans $V$. \\

   The standard basis of $F^n$ is \[
      ((1, 0, \hdots, 0), (0, 1, 0, \hdots, 0), \hdots, (0, \hdots, 0, 1))
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: A list $(v_1, \hdots, v_n)$ of vectors in $V$ is a basis if and only if every $v \in V$ can be written uniquely as 
   \[
     v = a_1 v_1 + \hdots + a_n v_n
   \] 
\end{framed}

\begin{framed}
   \textbf{Theorem}: Every spanning list in a vector space can be reduced to a basis of the vector space.  \\

   \textbf{Corollary}: Every finite dimensional vector space has a basis.
\end{framed}

\begin{framed}
   \textbf{Theorem}: Every linearly independent list of vectors in a finite dimensional vector space can be extended to form a basis. 
\end{framed}

\begin{framed}
   \textbf{Theorem}: If $V$ is finite dimensional and $U$ is a subspace of $V$, then there is a  subspace $W$ of $V$ such that $V = U \smalloplus W$.
\end{framed}

\section{Dimension}
\begin{framed}
   \textbf{Theorem}: Any two bases of a finite-dimensional vector space have the same length.
\end{framed}

\begin{framed}
   \textbf{Definition}: The \textbf{dimension} of a finite dimensional vector space is the length of any basis of the vector space.  
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $V$ is finite dimensional and $U$ is a subspace of $V$ 
   \[
     dim\ U \leq dim\ V
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $V$ is finite dimensional, every spanning list of vectors in $V$ of length $dim\ V$ is a basis of $V$
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $V$ is finite dimensional, every independent list of vectors in $V$ of length $dim\ V$ is a basis of $V$
\end{framed}

\begin{framed}
   \textbf{Theorem}: If $U_1$ and $U_2$ are subspace of a finite dimensional vector space, then 
   \[
     dim \left( U_1 + U_2  \right)  = dim\ U_1 + dim\ U_2 - dim \left( U_1 \cap U_2 \right) 
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: Suppose $V$ is finite dimensional and $U_1, \hdots, U_m$ are subspaces of $V$ such that
   \[
     V = U_1 + \hdots + U_m
   \] 
   \[
      dim\ V = dim\ U_1 + \hdots + dim\ U_m
   \] 

   Then
   \[
     V = U_1 \smalloplus \hdots \smalloplus U_m
   \] 
\end{framed}

\chapter{Linear Maps}

\section{Definition of linear map}
\begin{framed}
   \textbf{Definition}: A \textbf{linear map} from $V$ to $W$ is a function $T: V \rightarrow W$ with the following properties \\

   \textbf{Additivity}
   \[
     T(u + v) = Tu + Tv \text{ for all } u, v \in F
   \] 
   \textbf{Homogeneity}
   \[
     T(av) = a (Tv) \text{ for all } a \in F, v \in V
   \] 

   The set of all linear maps from $V$ to $W$ is denoted
   \[
      \mathcal{L}(V, W)
   \] 
\end{framed}

Examples of linear maps
\begin{itemize}
   \item zero
      \[
         0 \in \mathcal{L}(V, W) \text{ where } 0v = 0
      \] 
   \item identity
      \[
         I \in \mathcal{L}(V, W) \text{ where } Iv = v
      \] 
   \item differentiation i.e. $(f+g)' = f' + g'$ and $(af)' = af'$
      \[
         T \in \mathcal{L}(P(\mathbb{R}), P(\mathbb{R})) \text{ where }
         Tp = p'
      \] 
   \item integration 
      \[
         T \in \mathcal{L}(P(\mathbb{R}),\mathbb{R}) \text{ where }
         Tp = \int_{0}^{1} p(x) dx 
      \] 
   \item from $F^n$ to $F^m$, e.g.
      \[
         T \in \mathcal(L) \left( F^3, F^2 \right) \text{ where } T(x, y, z) = (a_{1, 1}x + a_{1, 2}y + a_{1, 3}z, a_{2, 1}x + a_{2, 2}y + a_{2, 3}z)
      \] 
\end{itemize}

$\mathcal(L)$ constitute a vector space if we define addition and scalar multiplication. \\

\begin{framed}
   \textbf{Definition}: \textbf{Addition of linear maps} is defined as
   \[
      (S + T) v = Sv + Tv
   \] 

   For $(S + T), S, T \in \mathcal{L}(V, W)$.
\end{framed}

\begin{framed}
   \textbf{Definition}: \textbf{Scalar multiplication of linear maps} is defined as
   \[
      (aT) v = a(Tv)
   \] 

   For $(aT), T \in \mathcal{L}(V, W), a \in F$.
\end{framed}

\begin{framed}
   \textbf{Definition}: \textbf{Product of linear maps} is defined as
   \[
      (ST)(v) = S(Tv)
   \] 

   \textbf{provided that }
   \[
      T \in \mathcal{L}(U, V), S \in \mathcal{L}(V, W)
   \] 

   Their product is then
   \[
      ST \in \mathcal{L}(U, W)
   \] 
   I.e. for some pairs of linear maps where a useful product exists, their product is their composition. \\

   Note that $ST$ is only defined when $T$ maps into the domain of $S$.  \\

   \textbf{Properties}
   \begin{itemize}
      \item Associativity
         \[
            (T_1 T_2) T_3 = T_1 (T_2 T_3)
         \] 
      \item Associativity
         \[
            TI = T \text{ and } IT = T \text{ for } T \in \mathcal{L}(V, W)
         \] 
      \item Distributive
         \[
            (S_1 + S_2) T = S_1 T + S_2 T
         \] 
         \[
           S(T_1 + T_2) = ST_1 + ST_2
         \] 
   \end{itemize}
\end{framed}

\section{Null spaces, ranges, injectivity and surjectivity}

\begin{framed}
   \textbf{Definition}: The \textbf{null space} of $T$ for $T \in \mathcal{L}(V, W)$ is the subset of $V$ consisting of those vectors that $T$ maps to $0$.
   \[
     null\ T = \{ v \in V: Tv = 0 \} 
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $T \in \mathcal{L}(V, W)$ then $null\ T$ is a subspace of $V$. 
\end{framed}

\begin{framed}
   \textbf{Definition}: A linear map $T: V \rightarrow W$ is injective if
    \[
     \forall u, v \in V, Tu = Tv \implies u = v
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: Let $T \in \mathcal{L}(V, W)$, $T$ is injective if and only if 
   \[
     null\ T = \{ 0 \} 
   \] 
\end{framed}
We only need to check whether $0$ is the only vector mapped to $0$ to show injectivity. \\


\begin{framed}
   \textbf{Definition}: For $T \in \mathcal{L}(V, W)$, the range of $T$  is the subset of $W$ consisting of those vectors that are of the form $Tv$ for some $v \in V$. 
    \[
     range\ T = \{ Tv: v \in V \} 
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition}: If $T \in \mathcal{L}(V, W)$ then the range of $T$ is a subspace of $W$. 
\end{framed}

\begin{framed}
   \textbf{Definition}: A linear map $T: V \rightarrow W$ is surjective if its range equals $W$. 
\end{framed}

\begin{framed}
   \textbf{Rank-Nullity Theorem}: If $V$ is finite dimensional and $T \in \mathcal{L}(V, W)$ then $range\ T$ is a finite dimensional subspace of $W$, and \textcolor{blue}{the sum of the dimension of the nullspace and the sum of the dimension of the range equals the dimension of the domain} . 
   \[
     dim\ V = dim\ null\ T + dim\ range\ T
   \] 
\end{framed}

\begin{framed}
   \textbf{Corollary to Rank-Nullity 1}: If $V$ and $W$ are finite dimensional and $dim\ V > dim\ W$ then no linear map from $V$ to $W$ is injective. 
\end{framed}

Proof for the corollary uses the fact that $dim\ null\ T > 0$, i.e. $null\ T$ contains vectors other than $0$. \\

\begin{framed}
   \textbf{Corollary to Rank-Nullity 2}: If $V$ and $W$ are finite dimensional and $dim\ V < dim\ W$ then no linear map from $V$ to $W$ is surjective. 
\end{framed}

\section{Matrix of a linear map}

\begin{framed}
   \textbf{Definition}: Let $T \in \mathcal{L}(V, W)$, and $ \left( v_1, \hdots, v_n \right) $ is a basis for $V$ and $ \left( w_1, \hdots w_m \right) $ is a basis for $W$. For each $k \in [1, n]$, 
    \[
       T v_k = a_{1, k} w_1 + \hdots + a_{m, k} w_m
   \] 

   The $m \times n$  matrix formed by $a_{j, k} $ is the \textbf{matrix} of $T$ 
   \[
     M \left( T, (v_1, \hdots v_n), (w_1, \hdots, w_m) \right) 
   \] 
\end{framed}

\textbf{Addition of matrices}
\[
   M(T + S) = M(T) + M(S) \text{ for } T, S \in \mathcal{L}(V, W)
\] 

\textbf{Scalar multiplication}
\[
   M(cT) = cM(T) \text{ for } c \in F 
\] 

\subsection{Vector space of matrices}

\begin{framed}
   \textbf{Definition}: The set of all $m \times n$ matrices with entries in $F$ constitutes a vector space, and the set is denoted
   \[
     Mat(m, n, F)
   \] 
\end{framed}

\textbf{Multiplying matrices}
Ideally we want 
\[
  M(TS) + M(T) M(S)
\] 

In order to satisfy this, we define the product of matrices as such. \\
\[
  M(T) = \begin{bmatrix} 
     a_{1, 1}&  \hdots &a_{1, n} \\
     \vdots&  & \vdots \\
     a_{m, 1}&  & a_{m, n}
  \end{bmatrix}
\] 

\[
  M(S) = \begin{bmatrix} 
     b_{1, 1}&  \hdots &b_{1, p} \\
     \vdots&  & \vdots \\
     b_{m, 1}&  & b_{n, p}
  \end{bmatrix}
\] 

Then $M(TS)$ is the $m \times p$ matrix whose entry in $j$ row and $k$ column is 
\[
   \sum_{r = 1}^n a_{j, r} b_{r, k}
\] 


\section{Invertibility}

\begin{framed}
   \textbf{Definition}: A linear map $T \in \mathcal{L}(V, W)$ is invertible if there exists a linear map $S \in \mathcal{L}(W, V)$ such that
   \[
      ST = I \text{ for } I \in \mathcal{L}(V, V)
   \] 
   \[
      TS = I \text{ for } I \in \mathcal{L}(W, W)
   \] 

   $S$ is the \textbf{inverse} of $T$.  \\

   If $T$ is invertible, then it has a unique inverse $T^{-1}$ \\
\end{framed}

\begin{framed}
   \textbf{Proposition}: A linear map is invertible if and only if it is injective and surjective.
\end{framed}

\begin{framed}
   \textbf{Definition}: Two vector spaces are \textbf{isomorphic} if there is an invertible linear map from one vector space onto another. 
\end{framed}

\begin{framed}
   \textbf{Theorem}: Two finite-dimensional vector space are isomorphic if and only if they have the same dimension.
  
\end{framed}

\begin{framed}
   \textbf{Proposition 3.19}: Suppose that $(v_1, \hdots , v_n)$ is a basis for $V$ and $(w_1, \hdots, w_m)$ is a basis of $W$, then $M$ is an invertible linear map between $\mathcal{L}(V, W)$ and $Mat(m, n, F)$.
\end{framed}

\textcolor{red}{This is a little confusing, come back and make sense of this} \\

This means there is a one-one correspondence between the matrix of a linear transformation from $V$ to $W$ and the set of all possible $m\ times n$ matrices with entries in $F$? \\

Consider the dimension of $Mat(m, n, F)$, the dimension can be found by choosing a basis. One of the possible bases is the set of $m \times n$ matrices with 0 in all entries except for a $1$ in one entry. There are $m \times n = mn$ such matrices. Hence $dim(Mat(m, n, F)) = mn$. \\

 \begin{framed}
    \textbf{Proposition}: If $V, W$ are finite dimensional, then 
    \[
       dim\ \mathcal{L}(V, W) = (dim\ V)(dim\ W)
    \] 
\end{framed}

\begin{framed}
   \textbf{Definition}: An \textbf{operator} is a linear map from a vector space onto itself. \\
   \[
      \mathcal{L} (V) = \mathcal{L}(V, V)
   \] 
\end{framed}

\begin{framed}
   \textbf{Theorem}: Suppose $V$ is finite-dimensional, if $T \in \mathcal{L}(V)$, then the following are equivalent
   \begin{enumerate}
      \item $T$ is invertible
      \item $T$ is injective
      \item $T$ is surjective
   \end{enumerate}

   \textcolor{red}{Note}: This only applies if $V$ is finite-dimensional. 
\end{framed}

\chapter{Polynomials}

\section{Degree}
\begin{framed}
   \textbf{Definition}: A function $p: F \rightarrow F$ is called a \textbf{polynomial} with coefficients in $F$ if there exists $a_0, \hdots, a_m \in F$  such that
   \[
     p(z) = a_0 + a_1 z + a_2 z^2 + \hdots + a_m z^m  \text{ for all } z \in F
   \] 

   If $p$ can be written in this form with $a_m \neq 0$, then $p$ has degree $m$. 
\end{framed}

\textcolor{red}{Note}: If $a_0 = a_1 = \hdots = a_m = 0$, then $p$ has degree $-\infty$ \\

Recall that $P(F)$ denotes the vector space of all polynomials with coefficients in $F$. \\

$P_m(F)$ denotes the subspace of $P(F)$ consisting of all polynomials with degree  \textbf{at most $m$}. \\

\begin{framed}
   \textbf{Definition}: A \textbf{root}, $\lambda$, of a polynomial is a number such that 
   \[
     p(\lambda) = 0
   \] 
\end{framed}

\begin{framed}
   \textbf{Proposition 4.1}: Suppose $p \in P(F)$ is a polynomial with degree $m \geq 1$, and  $\lambda \in F$, then $\lambda$ is a root if and of if there is a polynomial $q \in P(F)$ with degree $(m-1)$ such that
   \[
     p(z) = (z - \lambda) q(z) \text{ for all } z \in F
   \] 

   \textbf{Corollary}: Suppose $p \in P(F)$ is a polynomial with degree $ m \geq 0$, then  $p$ has at most $m$ distinct roots in $F$ \\

   \textbf{Corollary}: Suppose $a_0, \hdots a_m \in F$. If 
   \[
     a_0 + a_1 z + a_2 z^2 + \hdots + a_m z^m = 0
   \] 
   for all $z \in F$, then $a_0 = \hdots = a_m = 0$

   \textcolor{blue}{i.e. If a polynomial is identically 0, then all coefficients must be 0}
\end{framed}

By the last corollary, $(1, z, \hdots, z^m)$  is linearly independent in $P(F)$ (the only representation of $0$ is trivial). This linear independence implies each polynomial can be be represented in one way. 

\begin{framed}
   \textbf{Division Algorithm Lemma}: suppose $p, q \in P(F)$ with $p \neq 0$, then there exists polynomials  $s, r \in P(F)$ such that
    \[
     q = sp + r
   \] 
   and $deg\ r < deg\ p$
\end{framed}

\textcolor{red}{Revisit proof for this lemma}

\section{Complex Coefficients}

\begin{framed}
   \textbf{Fundamental Theorem of Algebra}: Every nonconstant polynomial with complex coefficients has a root.  \\

   \textbf{Corollary}: If $p \in P(C)$ is a nonconstant polynomial, then $p$ has a unique factorization (order irrelevant) of the form
   \[
     p(z) = c(z - \lambda_1) \hdots (z - \lambda_m) 
   \] 
   where $c, \lambda_1, \hdots , \lambda_m \in \textbf{C}$ 
\end{framed}

\section{Complex Numbers}

\begin{framed}
   \textbf{Definition}: For $z = a + bi$,  $a$ is the \textbf{real part} of $z$, $b$ is the \textbf{imaginary part} of $z$, 
   \[
     z = a + bi = Re\ z + (Im\ z)i
   \] 

   The \textbf{complex conjugate} of $z \in \textbf{C}$ is $\bar{z}$ 
   \[
      \bar{z} = Re\ z - (Im\ z) i
   \] 

   The \textbf{absolute value} of $z$ is $ \left| z \right| $ 
   \[
     \left| z  \right|  = \sqrt{(Re\ z)^2 + (Im\ z)^2}
   \] 
\end{framed}

Properties of complex numbers
\begin{itemize}
   \item additivity of real part
      \[
        Re(w + z) = Re\ w + Re\ z
      \] 
   \item additivity of imaginary part
      \[
        Im(w + z) = Im\ w + Im\ z
      \] 
   \item sum of $z$ and $\bar{z}$
      \[
         z + \bar{z} = 2Re\ z
      \] 
   \item difference of $z$ and $\bar{z}$
      \[
         z - \bar{z} = 2(Im\ z)i
      \] 
   \item product of $z$ and $\bar{z}$
      \[
         z  \bar{z} = |z|^2
      \] 
   \item additivity of complex conjugate
      \[
         \overline{w + z} = \bar{w} + \bar{z}
      \] 
   \item multiplicativity of complex conjugate
      \[
         \overline{w z} = \bar{w}\bar{z}
      \] 
   \item conjugate of conjugate
      \[
         \overline{\bar{z}} = z
      \] 
   \item multiplicativity of absolute value
      \[
         \left| wz \right|  = \left| w \right| \left| z \right| 
      \] 
\end{itemize}

\section{Real Coefficients}
\begin{framed}
   \textbf{Proposition}: Suppose $p$ is a polynomial with real coefficients, if $\lambda \in C$ is a root, then so is $\bar{\lambda}$
\end{framed}

\begin{framed}
   \textbf{Proposition}: Let $\alpha, \beta \in \mathbb{R}$, then there is a polynomial factorization of the form
   \[
     x^2 + \alpha x + \beta = (x - \lambda_1) (x - \lambda_2)
   \] 
   with $\lambda_1, \lambda_2 \in \mathbb{R}$ if and only if $\alpha^2 \geq 4 \beta$
\end{framed}

\begin{framed}
   \textbf{Theorem}: If $p \in P(\mathbb{R})$ is a nonconstant polynomial, then $p$ has a unique factorization of the form
   \[
     p(x) = c(x - \lambda_1) \hdots (x - \lambda_m) (x_2 + \alpha_1 x + \beta_1) \hdots (x^2 + \alpha_Mx + \beta_M)
   \] 
   where $c, \lambda_1, \hdots, \lambda_m \in \mathbb{R}$ and $(\alpha_1, \beta_1), \hdots (\alpha_M, \beta_M) \in \mathbb{R}^2$ with $\alpha_j^2 < 4\beta_j$
\end{framed}

\chapter{Eigenvalues and eigenvectors}

Eigenvalues and eigenvectors are concerned with linear maps from a vector space to itself. This constitutes the deepest and most important part of linear algebra. \\ 

\section{Invariant subspaces}


For some operator $T \in \mathcal{L}(V)$, assuming $V$ can be direct sum decomposed into
\[
  V = U_1 \smalloplus \hdots \smalloplus U_m
\]
the behavior of $T$ can be understood by considering the behavior of $\left. T \right|_{U_j}^{} $, i.e. $T$ restricted to the domain of $U_j$. However,  $\left. T \right|_{U_j}^{} $ might not be an operator (i.e. might not map $U_j$ to $U_j$. This is the motivating example for studying invariant subspaces. \\

\begin{framed}
   \textbf{Definition}: For $T \in \mathcal{L}(V)$ and $U$ a subspace of $V$, $U$ is \textbf{invariant} under $T$ if $u \in U $ implies  $Tu \in U$ i.e.
   \[
     u \in U \implies Tu \in U
   \] 

   i.e. $U$ is invariant under $T$ if
   \[
      \left. T \right|_{U}^{}  \in \mathcal{L}(U)
   \] 
\end{framed}

Examples of invariant subspaces
\begin{itemize}
   \item null space
       \[
          T \in \mathcal{L}(V) \implies null\ T \text{ invariant under $T$}
      \] 
   \item range
       \[
          T \in \mathcal{L}(V) \implies range\ T \text{ invariant under $T$}
      \] 
\end{itemize}

\subsection{One dimensional invariant subspaces}
Subspaces of $V$ of dimension 1 can be found by taking \textbf{any} $u \in V$, and finding the set of all scalar multiples of $u$ 
\[
  U = \{ au : a \in F \} 
\] 

Every one dimensional  subspace of $V$ has the same form. \\

If $U$ is invariant under some $T \in \mathcal{L}(V)$, then by definition
 \[
  Tu \in U \implies Tu = \lambda u \text{ for some } \lambda \in F
\] 

\begin{framed}
   \textbf{Definition}: A scalar $\lambda \in F$ is an \textbf{eigenvalue} of $T \in \mathcal{L}(V)$ if there exists a nonzero vector  $u \in V$ such that
   \[
     Tu = \lambda u  \text{ or } (T - \lambda I) u = 0
   \] 

   \textbf{Corollary}: $T$ has a one dimensional invariant subspace if and only if $T$ has an eigenvalue. 
\end{framed}

Note that
\begin{align*}
   & Tu = \lambda u \\
   & \iff (T - \lambda I) u = 0 \\
   & \iff (T - \lambda I)\text{ is not injective} \\
   & \iff (T - \lambda I)\text{ is not invertible} \\
   & \iff (T - \lambda I)\text{ is not surjective} 
\end{align*}

\begin{framed}
   \textbf{Definition}: Suppose $T \in \lambda{L}(V)$ and $\lambda \in F$ is an eigenvalue of $T$, a vector $u \in V$ is called an \textbf{eigenvector} of $T$ if 
   \[
     Tu = \lambda u
   \] 

   \textbf{Corollary}: Since $(T - \lambda I) u = 0$, the set of eigenvectors of  $T$ corresponding to $\lambda$ is exactly
    \[
    null (T - \lambda I)
   \] 
\end{framed} 

\begin{framed}
   \textbf{Theorem}: Let $T \in \mathcal{L}(V) $. Suppose  $\lambda_1, \hdots, \lambda_m$ are distinct eigenvalues of $T$ and $v_1, \hdots, v_m$ are corresponding nonzero eigenvectors. Then $(v_1, \hdots, v_m)$ is linearly independent. 

   \textbf{Corollary}: Each operator on $V$ has at most $dim\ V$ distinct eigenvalues. 
\end{framed}

\section{Polynomials applied to operators}

Unlike linear maps, operators can be raised to powers. If $T \in \mathcal{L}(V)$ then  $T^2 = TT \in \mathcal{L}(V)$.  \\

\[
  T^m = T \hdots T
\] 
If $T$ is invertible, then inverse of $T$ is $T^{-1}$, and
\[
   T^{-m} = \left(T^{-1}\right)^m
\]
Since $T$ is an operator
\[
   T^m T^n = T^{m + n} \text{ and } (T^m)^n = T^{mn}
\] 

We can take the polynomial of an operator, 
\[
  p(T) = a_0 I + a_1 T + a_2 T^2 + \hdots + a_m T^m
\] 

If $p$ and  $q$ are polynomials with coefficients in $F$, then 
\[
   (pq)T = p(T) q(T)
\] 

\section{Upper triangular matrices}
\begin{framed}
\textbf{Theorem}: Every operator on a finite dimensional, non-zero complex vector space has an eigenvalue. 
\end{framed}

Since operators map a vector space onto itself, we only need to consider one basis, and matrices of operators will always be \textbf{square arrays}. \\

Let $T \in \mathcal{L}(V)$, suppose  $(v_1, \hdots, v_n)$ is a basis for $V$, then 
\[
   Tv_k = a_{1, k}v_1  + \hdots + a_{n, k} v_n
\] 
and the matrix of $T$ with respect to  $(v_1, \hdots, v_n)$ is 
\[
  M(T, (v_1, \hdots, v_n)) = M(T) = \begin{bmatrix} 
     a_{1, 1}   & \hdots & a_{1, n} \\
     \vdots& & \vdots \\
     a_{n, 1}   & \hdots & a_{n, n}
  \end{bmatrix}
\] 
 
A central goal of linear algebra is to show that given an operator $T$, \textcolor{blue}{there exists a basis with respect to which $T$ has a reasonable simple matrix}. \\ 

\textcolor{red}{If $V$ is a complex vector space and $T \in \mathcal{L}(V)$, then there is a basis of  $V$ with respect to which $M(T)$ has the form} 
 \[
  \begin{bmatrix} 
     \lambda & & \\
     0 & * & \\
     \vdots & & \\
     0 & &
  \end{bmatrix}
\] 

This is because since $T $ has an eigenvalue $\lambda$ and a corresponding eigenvector $v$,  $v$ can be extended to a basis of $V$. \\

From definition of eigenvectors
\[
   Tv = \lambda v
\] 
From definition of the matrix representation of $T$ 
\[
   Tv = a_{1, 1} v + \hdots + a_{n, 1}v_n
\] 

\begin{framed}
   \textbf{Proposition}: Suppose $T \in \mathcal{L}(V)$  and $(v_1, \hdots, v_n)$ is a basis of $V$, then 
   \begin{enumerate}
      \item the matrix of $T$ with respect to $(v_1, \hdots, v_n)$ is upper triangular
      \item $Tv_k \in span(v_1, \hdots, v_k)$ for each $k = 1, \hdots, n$ 
      \item  $span(v_1, \hdots, v_k)$ is invariant under $T$ for each $k = 1, \hdots, n$
   \end{enumerate}
\end{framed}

\textcolor{blue}{Notes for the above equivalence}:  \\

(1) and (2): If $M(T)$ is upper triangular, since we know that the $k$-th column of $M$ acts on the  $k$-th basis, i.e.
\[
   Tv_k = a_{1, k} v_1 + a_{2, k} v_2 +  \hdots + a_{n, k} v_n
\] 

If $M(T)$ is upper triangular, then only $a_{1, k}, \hdots, a_{k, k}$ are non-zero. Hence
\begin{align*}
   Tv_k &= a_{1, k} v_1 + a_{2, k} v_2 +  \hdots + a_{k, k} v_k + 0 \times v_{k + 1} + \hdots + 0 \times v_{n} \\
        &= a_{1, k} v_1 + a_{2, k} v_2 +  \hdots + a_{k, k} v_k
\end{align*}

Hence $T v_k \in span(v_1, \hdots, v_k)$ \\

(2) and (3): Recall that  $U$ is invariant under $T$ if $u \in U$ implies  $Tu \in U$, 
 \begin{align*}
    \forall v \in span(v_1, \hdots, v_k), Tv \in span(v_1, \hdots, v_k)
\end{align*}

\textcolor{red}{Note}: The lines above do not constitute a proof. Refer to the book for the full proof.

\begin{framed}
   \textbf{Theorem}: Suppose $V$ is a complex vector space and $T \in \mathcal{L}(V)$, then  $T$ has an upper triangular matrix with respect to some basis of $V$. 
\end{framed}

\textcolor{red}{Note}: This theorem does not apply to real vector spaces. The first vector in a basis for $V$ with respect to which $T$ has an upper triangular matrix must be an eigenvector. \\

The theorem above only guarantees the existence of an eigenvector for a non-zero complex vector space. 

\begin{framed}
   \textbf{Proposition 5.16}: Suppose $T \in \mathcal{L}(V)$ has an upper triangular matrix with respect to some basis of $V$, then $T$ is invertible if and only if \textbf{all} entries on the diagonal of the upper triangular matrix are non-zero. 
\end{framed}

It would be good if we could compute the eigenvalues of an operator from its matrix exactly. However, \textcolor{red}{no such method exists}.\\

If we could find a basis with which the operator is upper triangular, then the computation of eigenvalues is trivial. \\


\begin{framed}
   \textbf{Proposition 5.18}: Suppose $T \in \mathcal{L}(V)$ has an upper triangular matrix with respect to some basis of  $V$, then the eigenvalues of $T$ consists precisely of the entries on the diagonal of the upper triangular matrix. 
\end{framed}

Let $(v_1, \hdots, v_n)$ be a basis for $V$ with respect to which $T$ has an upper triangular matrix
\[
  M(T, (v_1, \hdots, v_n)) = \begin{bmatrix} 
     \lambda_1 & & & * \\  
     & \lambda_2& &  \\  
     & & \ddots &  \\  
     0 & &  & \lambda_n 
  \end{bmatrix}
\] 

Let $\lambda \in F$, then
\[
  M(T - \lambda I, (v_1, \hdots, v_n)) = \begin{bmatrix} 
     \lambda_1 - \lambda & & & * \\  
     & \lambda_2 - \lambda& &  \\  
     & & \ddots &  \\  
     0 & &  & \lambda_n  - \lambda
  \end{bmatrix}
\] 

By definition of eigenvalues and eigenvectors, $\lambda$ is a eigenvalue of $T$ if there exists some $u \in V$ such that
\[
  Tu = \lambda u \text{ or }  (T - \lambda I) u = 0 \text{ or } T - \lambda I \text{ not invertible}
\] 

$T - \lambda I$ is not invertible if and only if $\lambda$ equals one of the $\lambda_j$ . 

\section{Diagonal matrices}
\begin{framed}
   \textbf{Definition}: A \textbf{diagonal matrix} is a square matrix with $0$ everywhere except possibly along the diagonal.  \\

   An operator $T \in \mathcal{L}(V)$ has a diagonal matrix with respect to some basis $(v_1, \hdots, v_n)$ if and only if
   \begin{align*}
      Tv_1 &= \lambda_1 v_1 \\
           & \hdots \\
      Tv_n &= \lambda_n v_n
   \end{align*}
\end{framed}

\textcolor{blue}{i.e.} An operator $T$ has a diagonal matrix with respect to some basis $V$ if and only if $V$ has a basis consisting of eigenvectors of $T$.  \\

\textcolor{red}{Note}: Not every operator has a diagonal matrix with respect to some basis. Even though every operator on a (finite) complex vector spaces has a eigenvector, there may not be enough linearly independent eigenvectors of $T$ to form a basis for $V$.

\begin{framed}
   \textbf{Proposition 5.20}: If $T \in \mathcal{L}(V)$ has  $dim\ V$ distinct eigenvalues, then $T$ has a diagonal matrix with respect to some basis $V$. 
\end{framed}

\textcolor{red}{Note}: the converse of this is not true. Operators with fewer eigenvalues than the dimension of the domain may also have diagonal matrices. 

\begin{framed}
   \textbf{Proposition 5.21}: Suppose $T \in \mathcal{L}(V)$, and  $\lambda_1, \hdots, \lambda_{m}$,  are distinct eigenvalues of $T$, then the following are equivalent
   \begin{enumerate}
      \item $T$ has a diagonal matrix with respect to some basis of $V$
      \item $V$ has a basis consisting of eigenvectors of $T$
      \item there exist one-dimensional subspaces  $U_1, \hdots, U_n$ of $V$, each invariant under $T$ such that
         \[
           V = U_1 \smalloplus \hdots \smalloplus U_n
         \] 
      \item $V = null(T - \lambda_1 I) \smalloplus \hdots \smalloplus null (T - \lambda_m I)$
      \item $dim\ V = dim\ null(T - \lambda_1) + \hdots + dim\ null (T - \lambda_m I)$
   \end{enumerate}
\end{framed}

\section{Invariant subspaces on real vector spaces}

\begin{framed}
   \textbf{Theorem 5.24}: Every operator on a finite dimensional, nonzero, real vector space has an invariant subspace of dimension $1$ or $2$. 
\end{framed}

\begin{framed}
   \textbf{Definition}: Suppose $U$ and $W$ are subspaces of $V$ such that 
      \[
        V = U \smalloplus W
      \] 

      Each vector $v \in V$ can be written as 
      \[
        v = u + w
      \] 
      where $u \in U$ and $w \in W$. We define a \textbf{projection} operator $P_{U, W} \in \mathcal{L}(V)$ such that
      \[
         P_{U, W} v = u
      \] 
\end{framed}

\textcolor{red}{Note}: $P_{U, W} $ is the projection onto $U$ with nullspace of $W$. 

Properties of projection
\begin{itemize}
   \item
      \[
         v = P_{U, W} v + P_{W, U} v
      \] 
   \item 
      \[
         P_{U, W}^2 = P_{U, W}
      \] 
   \item range of $P$ 
      \[
         range\ P_{U, W} = U
      \] 
   \item kernel of $P$ 
      \[
         null\ P_{U, W} = W
      \] 
\end{itemize}

\begin{framed}
   \textbf{Theorem 5.26}: Every operator on an odd-dimensional real vector space has an eigenvalue. 
\end{framed}

\chapter{Inner product spaces}
\section{Inner product}
\begin{framed}
   \textbf{Definition}: The 'length' of a vector is called the \textbf{norm} of $x$, denoted $ \lVert x \rVert $. 
   \[
     \lVert x \rVert  = \sqrt{x_1^2 + \hdots + x_n^2}
   \] 
\end{framed}

Note that the norm is not linear on $\textbf{R}^n$

\begin{framed}
   \textbf{Defintion}: For $x, y in \textbf{R}^n$, the \textbf{dot product } of $x, y $ is
   \[
     x \cdot y = x_1y_1 + \hdots + x_n y_n
   \] 
\end{framed}

Note that
\begin{itemize}
   \item dot product returns a number, not vector
   \item $x \cdot x \geq 0$  for all  $x \in \textbf{R}^n$
   \item $x \cdot x = 0$ if and only if  $x = 0$ \\
   \item if $y \in \textbf{R}^n$ is fixed, then map from $R^n \text{ to} \textbf{R}$  is linear
   \item $x \cdot y = y \cdot x$
\end{itemize}

\begin{framed}
   \textbf{Definition}: An \textbf{inner product } on $V$ is a function that takes each ordered pair $(u, v)$ of elements in $V$ to a number $ \langle u, v\rangle \in F$ with the following properties
   \begin{itemize}
      \item positivity
         \[
            \langle v, v \rangle \geq 0 \text{ for all $v \in V$}
         \] 
      \item definiteness
         \[
            \langle v, v \rangle = 0 \text{ if and only if $v = 0$}
         \] 
      \item additivity in first slot
         \[
            \langle u = v, w \rangle = \langle u, w \rangle + \langle v, w \rangle \text{ for all } u, v, w \in V
         \] 
      \item homogeneity in first slot
         \[
            \langle av, w \rangle = a \langle v, w \rangle
         \] 
      \item conjugate symmetry
         \[
            \langle av, w \rangle = \overline{ \langle w, v\rangle}
         \] 
   \end{itemize}
\end{framed}

\begin{framed}
   \textbf{Definition}: An inner product space is vector space $V$ along with an inner product on $V$. 
\end{framed}



\section{Norm}
\section{Orthonormal bases}
\section{Orthogonal projections and minimization problems}
\section{Linear functions and adjoints}

\chapter{Operators on inner product spaces}
\section{Self adjoint and normal operators}
\section{Spectral theorem}
\section{Normal operators on real inner product spaces}
\section{Positive operators}
\section{Isometries}
\section{Polar and singular value decompositions}

\chapter{Operators on complex vector spaces}
\chapter{Operators on real vector spaces}
\chapter{Trace and determinant}

\end{document}
